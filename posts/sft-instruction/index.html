<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Supervised finetuning of instruction-following LLMs</title><meta name=description content="Daniel Furman's Portfolio and Blog"><meta name=author content="Daniel Furman"><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous><link rel=stylesheet href=/sass/researcher.min.css><link rel=icon type=image/ico href=https://daniel-furman.github.io/favicon.ico></head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><body><div class="container mt-5"><nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0"><a class="navbar-brand mx-0 mr-sm-auto" href=https://daniel-furman.github.io/>Daniel Furman</a><div class="navbar-nav flex-row flex-wrap justify-content-center"><a class="nav-item nav-link" href=/>About</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/posts>Posts</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/bookshelf>Bookshelf</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/resume>Resume</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/contact>Contact</a></div></nav></div><hr><div id=content><div class=container><h1 id=supervised-finetuning-of-instruction-llms>Supervised finetuning of instruction LLMs</h1><hr><p>Post corresponds to the README from this <a href=https://github.com/daniel-furman/sft-demos>repo</a>.</p><hr><p>This repo contains demos for supervised finetuning (sft) of large language models, like MosaicML&rsquo;s <a href=https://huggingface.co/mosaicml/mpt-7b>mpt</a> and Meta&rsquo;s <a href=https://huggingface.co/meta-llama/Llama-2-7b-hf>llama-2</a>. In particular, we focus on short-form instruction following.</p><h2 id=instruction-tuning-background>Instruction tuning background</h2><p>In recent years, there has been a growing interest in building models that can follow natural language instructions to perform a wide range of tasks. These models, known as &ldquo;instruction-tuned&rdquo; language models, have demonstrated the ability to generalize to new tasks.</p><p>The below was captured from the &ldquo;<a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">State of GPTs</a>&rdquo; talk by Andrej Karpathy. The key points illustrated for SFT:</p><ul><li>Collect small but high-quality datasets in the form of &ldquo;prompt&rdquo; and &ldquo;ideal responses&rdquo;.</li><li>Do language modeling on this data, nothing changes algorithmically from pretraining.</li><li>After training we get an SFT model which can be deployed as assistants (and it works to some extent).</li></ul><p><img src=/posts/sft-assets/assistant_training_pipeline.png alt=training_pipeline></p><p>For more background, see any number of excellent papers on the subject, including <a href=https://arxiv.org/pdf/2212.10560.pdf>Self-Instruct</a> (2023), <a href=https://arxiv.org/pdf/2306.02707.pdf>Orca</a> (2023), and <a href=https://arxiv.org/pdf/2203.02155.pdf>InstructGPT</a> (2022).</p><h2 id=code-assets>Code assets</h2><ul><li>See the <code>./sft</code> folder for finetuning scripts and postprocessing notebooks.</li><li>See the <code>./runs</code> folder for the raw results from each sft experiment.</li><li>See the <code>./inf_tests</code> folder for runtime testing notebooks.</li></ul><h2 id=base-models-and-datasets>Base models and datasets</h2><p>In this repo, we finetune the following base models. Each is open-source and licensed for commercial use.</p><ul><li><a href=https://huggingface.co/mosaicml/mpt-7b>mpt</a></li><li><a href=https://huggingface.co/meta-llama/Llama-2-7b-hf>llama-2</a></li></ul><p>We test the following datasets. Each is open-source and licensed for commercial use.</p><ul><li><a href=https://huggingface.co/datasets/timdettmers/openassistant-guanaco>timdettmers/openassistant-guanaco</a></li><li><a href=https://huggingface.co/datasets/ehartford/dolphin>ehartford/dolphin</a></li></ul><h2 id=finetuned-models>Finetuned models</h2><h3 id=1-llama-2-70b-dolphin-pefthttpshuggingfacecodfurmanllama-2-70b-dolphin-peft>1. <a href=https://huggingface.co/dfurman/llama-2-70b-dolphin-peft><code>llama-2-70b-dolphin-peft</code></a></h3><p>This instruction model was built via parameter-efficient QLoRA finetuning of <a href=https://huggingface.co/meta-llama/Llama-2-70b-hf>llama-2-70b</a> on the first 25k rows of <a href=https://huggingface.co/datasets/ehartford/dolphin>ehartford/dolphin</a> (an open-source implementation of <a href=https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/>Microsoft&rsquo;s Orca</a>). Finetuning was executed on a single H100 (80 GB PCIe) for roughly 17 hours on the <a href=https://cloud.lambdalabs.com/instances>Lambda Labs</a> platform.</p><ul><li>Model license: Llama 2 Community License Agreement</li><li>Basic usage: <a href=https://huggingface.co/dfurman/llama-2-70b-dolphin-peft/blob/main//posts/sft-assets/basic_inference_llama_2_70b_dolphin.ipynb>notebook</a></li><li>Finetuning code: <a href=https://github.com/daniel-furman/sft-demos/blob/main/src/sft/one_gpu/llama-2/dolphin/sft-llama-2-70b-dolphin-peft.py>script</a></li></ul><p><img src=/posts/sft-assets/jul_24_23_1_14_00_log_loss_curves_llama-2-70b-dolphin.png alt=loss_curves></p><p>The above loss curve was generated from the run&rsquo;s private wandb.ai log.</p><h3 id=example-prompts-and-responses>Example prompts and responses</h3><p>Example 1:</p><p><strong>User</strong>:</p><blockquote><p>You are a helpful assistant. Write me a numbered list of things to do in New York City.\n</p></blockquote><p><strong>llama-2-70b-dolphin-peft</strong>:</p><blockquote><ol><li>Visit the Statue of Liberty: Take a ferry to Liberty Island and see the iconic symbol of freedom and democracy.</li><li>Explore Central Park: Enjoy the lush greenery, take a stroll, or rent a bike to explore the park&rsquo;s many attractions, such as the Central Park Zoo and Strawberry Fields.</li><li>Visit the Empire State Building: Take an elevator to the observation deck for breathtaking views of the city.</li><li>Walk the Brooklyn Bridge: Cross the historic bridge and enjoy the stunning views of the Manhattan skyline.</li><li>Experience Times Square: Take in the bright lights, billboards, and bustling energy of this iconic intersection.</li><li>Visit the Metropolitan Museum of Art: Explore the vast collection of art and artifacts from around the world.</li><li>Take a food tour: Sample the diverse cuisine of New York City, from pizza to bagels to ethnic specialties.</li><li>Attend a Broadway show: Enjoy a world-class performance in the heart of the Theater District.</li></ol></blockquote><br><p>Example 2:</p><p><strong>User</strong>:</p><blockquote><p>You are a helpful assistant. Write a short email inviting my friends to a dinner party on Friday. Respond succinctly.\n</p></blockquote><p><strong>llama-2-70b-dolphin-peft</strong>:</p><blockquote><p>Subject: Dinner Party Invitation - Friday, 7 PM</p><p>Dear Friends,</p><p>I hope this email finds you well! I&rsquo;m excited to invite you to a dinner party at my place this Friday, at 7 PM. We&rsquo;ll have a delicious meal, great company, and lots of fun. Please let me know if you can make it, so I can plan accordingly.</p><p>Looking forward to seeing you all!</p><p>Kind regards,
Your Name</p></blockquote><blockquote><p>P.S. Please let me know if you have any dietary restrictions or allergies.</p></blockquote><br><p>Example 3:</p><p><strong>User</strong>:</p><blockquote><p>You are a helpful assistant. Tell me a recipe for vegan banana bread.\n</p></blockquote><p><strong>llama-2-70b-dolphin-peft</strong>:</p><blockquote><p>Sure! Here&rsquo;s a delicious and easy vegan banana bread recipe:</p><p>Ingredients:</p><ul><li>2 cups all-purpose flour</li><li>1/2 cup sugar</li><li>1/2 cup vegan butter (such as Earth Balance)</li><li>1/2 cup vegan milk (such as almond milk)</li><li>1/2 cup unsweetened applesauce</li><li>1/2 cup mashed ripe bananas (about 2 medium bananas)</li><li>1 teaspoon baking soda</li><li>1/2 teaspoon salt</li><li>1/2 teaspoon ground cinnamon</li><li>1/2 teaspoon ground nutmeg</li><li>1/2 teaspoon ground cloves</li><li>1/2 cup chopped walnuts (optional)</li></ul><p>Instructions:</p><ol><li>Preheat the oven to 350°F (175°C). Grease a 9x5-inch loaf pan with vegan butter or cooking spray.</li><li>In a large bowl, mix together the flour, sugar, vegan butter, vegan milk, applesauce, bananas, baking soda, salt, cinnamon, nutmeg, and cloves. Stir until well combined.</li><li>Fold in the chopped walnuts, if using.</li><li>Pour the batter into the prepared loaf pan.</li><li>Bake for 50-60 minutes, or until a toothpick inserted into the center of the bread comes out clean.</li><li>Let the bread cool in the pan for 10 minutes before transferring it to a wire rack to cool completely.</li><li>Slice and enjoy!</li></ol><p>Note: You can also add chocolate chips, dried fruit, or other mix-ins to the batter for extra flavor and texture. Enjoy your vegan banana bread!</p></blockquote><br><h3 id=runtime-tests>Runtime tests</h3><table><thead><tr><th style=text-align:center>runtime / 50 tokens (sec)</th><th style=text-align:center>GPU</th><th style=text-align:center>attn</th><th style=text-align:center>torch dtype</th><th style=text-align:center>VRAM (GB)</th></tr></thead><tbody><tr><td style=text-align:center>4.50</td><td style=text-align:center>1x H100 (80 GB PCIe)</td><td style=text-align:center>torch</td><td style=text-align:center>nf4</td><td style=text-align:center>39</td></tr></tbody></table><p>The above runtime stats were generated from this <a href=https://github.com/daniel-furman/sft-demos/blob/main/src/sft/one_gpu/llama-2/dolphin/postprocessing-llama-2-70b-dolphin-peft.ipynb>notebook</a>.</p><br><h3 id=2-llama-2-13b-guanaco-pefthttpshuggingfacecodfurmanllama-2-13b-guanaco-peft>2. <a href=https://huggingface.co/dfurman/llama-2-13b-guanaco-peft><code>llama-2-13b-guanaco-peft</code></a></h3><p>This chatbot model was built via parameter-efficient QLoRA finetuning of <a href=https://huggingface.co/meta-llama/Llama-2-13b-hf>llama-2-13b</a> on all 9.85k rows of <a href=https://huggingface.co/datasets/timdettmers/openassistant-guanaco>timdettmers/openassistant-guanaco</a> (a subset of <a href=https://huggingface.co/datasets/OpenAssistant/oasst1>OpenAssistant/oasst1</a> containing the highest-rated conversation paths). Finetuning was executed on a single A6000 (48 GB) for roughly 3.7 hours on the <a href=https://cloud.lambdalabs.com/instances>Lambda Labs</a> platform.</p><ul><li>Model license: Llama 2 Community License Agreement</li><li>Basic usage: <a href=https://huggingface.co/dfurman/llama-2-13b-guanaco-peft/blob/main//posts/sft-assets/basic_inference_llama_2_13b_guanaco.ipynb>notebook</a></li><li>Finetuning code: <a href=https://github.com/daniel-furman/sft-demos/blob/main/src/sft/one_gpu/llama-2/guanaco/sft-llama-2-13b-guanaco-peft.ipynb>notebook</a></li></ul><p><img src=/posts/sft-assets/jul_22_23_3_15_00_log_loss_curves_llama-2-13b-guanaco.png alt=loss_curves></p><p>The above loss curve was generated from the run&rsquo;s private wandb.ai log.</p><h3 id=example-prompts-and-responses-1>Example prompts and responses</h3><p>Example 1:</p><p><strong>User</strong>:</p><blockquote><p>### Human: Write me a numbered list of things to do in New York City.### Assistant:</p></blockquote><p><strong>llama-2-13b-guanaco-peft</strong>:</p><blockquote><ol><li>Visit the Statue of Liberty and Ellis Island</li><li>Take a stroll through Central Park</li><li>Visit the Empire State Building</li><li>See a Broadway show</li><li>Explore the Metropolitan Museum of Art</li><li>Visit the 9/11 Memorial and Museum</li><li>Take a ride on the Staten Island Ferry</li><li>Shop on Fifth Avenue</li><li>Eat a slice of pizza at a local pizzeria</li><li>Visit the High Line</li></ol></blockquote><br><p>Example 2:</p><p><strong>User</strong>:</p><blockquote><p>### Human: Write a short email inviting my friends to a dinner party on Friday. Respond succinctly.### Assistant:</p></blockquote><p><strong>llama-2-13b-guanaco-peft</strong>:</p><blockquote><p>Subject: Dinner party on Friday</p><p>Hey guys,</p><p>I&rsquo;m hosting a dinner party on Friday at my place. It&rsquo;s going to be a small gathering with just a few of us. I&rsquo;ll be cooking some delicious food, so I hope you can make it.</p><p>Let me know if you can come and what you&rsquo;d like to bring. I&rsquo;ll send out the address and more details later.</p><p>See you soon!</p><p>[Your name]</p></blockquote><br><h3 id=runtime-tests-1>Runtime tests</h3><table><thead><tr><th style=text-align:center>runtime / 50 tokens (sec)</th><th style=text-align:center>GPU</th><th style=text-align:center>attn</th><th style=text-align:center>torch dtype</th><th style=text-align:center>VRAM (GB)</th></tr></thead><tbody><tr><td style=text-align:center>2.93</td><td style=text-align:center>1x A100 (40 GB SXM)</td><td style=text-align:center>torch</td><td style=text-align:center>bfloat16</td><td style=text-align:center>25</td></tr><tr><td style=text-align:center>3.24</td><td style=text-align:center>1x A6000 (48 GB)</td><td style=text-align:center>torch</td><td style=text-align:center>bfloat16</td><td style=text-align:center>25</td></tr></tbody></table><p>The above runtime stats were generated from this <a href=https://github.com/daniel-furman/sft-demos/blob/main/src/sft/one_gpu/llama-2/guanaco/postprocessing-llama-2-13b-guanaco-peft.ipynb>notebook</a>.</p><br><h3 id=3-mpt-7b-dolphinhttpshuggingfacecodfurmanmpt-7b-dolphin>3. <a href=https://huggingface.co/dfurman/mpt-7b-dolphin><code>mpt-7b-dolphin</code></a></h3><p>This instruction model was built via full parameter finetuning of <a href=https://huggingface.co/mosaicml/mpt-7b>mpt-7b</a> on the first 100k rows of <a href=https://huggingface.co/datasets/ehartford/dolphin>ehartford/dolphin</a> (an open-source implementation of <a href=https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/>Microsoft&rsquo;s Orca</a>). Finetuning was executed on a single H100 (80 GB PCIe) for roughly 12 hours on the <a href=https://cloud.lambdalabs.com/instances>Lambda Labs</a> platform.</p><ul><li>Model license: Apache 2.0</li><li>Basic usage: <a href=https://huggingface.co/dfurman/mpt-7b-dolphin/blob/main//posts/sft-assets/basic_inference_mpt-7b-dolphin.ipynb>notebook</a></li><li>Finetuning code: <a href=https://github.com/daniel-furman/sft-demos/blob/main/src/sft/one_gpu/mpt/dolphin/sft-mpt-7b-dolphin.py>script</a></li></ul><p><img src=/posts/sft-assets/jul_5_23_3_15_00_log_loss_curves_mpt-7b-dolphin.png alt=loss_curves></p><p>The above loss curve was generated from the tfevents log at <a href=https://github.com/daniel-furman/sft-demos/tree/main/runs/jul_5_23_3_15_00_sft-mpt-7b-dolphin>link</a>.</p><h3 id=example-prompts-and-responses-2>Example prompts and responses</h3><p>Example 1:</p><p><strong>User</strong>:</p><blockquote><p>You are a helpful assistant. Write me a numbered list of things to do in New York City.\n</p></blockquote><p><strong>mpt-7b-dolphin</strong>:</p><blockquote><ol><li>Visit the Statue of Liberty: The Statue of Liberty is a symbol of freedom and democracy, and it is a must-see attraction in New York City.</li><li>Take a walk in Central Park: Central Park is a massive green space in the heart of Manhattan, and it is a great place to relax, enjoy nature, and people-watch.</li><li>Visit the Metropolitan Museum of Art: The Met is one of the world&rsquo;s largest and most renowned art museums, housing an extensive collection of art from around the world.</li><li>Take a stroll through Times Square: Times Square is the heart of the city, and it is a must-see for its iconic billboards, bright lights, and bustling energy.</li><li>Visit the Empire State Building: The Empire State Building is a famous skyscraper in Midtown Manhattan, offering panoramic views of the city from its observation deck.</li><li>Explore the Museum of Modern Art (MoMA): MoMA is a renowned art museum in Midtown Manhattan, showcasing modern and contemporary art from around the world.</li><li>Visit the 9/11 Memorial and Museum: The 9/11 Memorial and Museum is a somber but important attraction in Lower Manhattan, commemorating the victims of the September 11th attacks.</li><li>Take a ferry ride to the Statue of Liberty: A ferry ride to the Statue of Liberty is a great way to see the city from the water</li></ol></blockquote><br><p>Example 2:</p><p><strong>User</strong>:</p><blockquote><p>You are a helpful assistant. Write a short email inviting my friends to a dinner party on Friday. Respond succinctly.\n</p></blockquote><p><strong>mpt-7b-dolphin</strong>:</p><blockquote><p>Subject: Invitation to Dinner Party on Friday</p><p>Dear Friend,</p><p>I hope this email finds you well. I am writing to invite you to a dinner party that I am hosting on Friday. The party will be a casual gathering, and I would love to have you join us.</p><p>The dinner will be held at my home, and the time is from 7:00 pm to 10:00 pm. The address is 123 Main Street, San Francisco, CA 94122.</p><p>The dinner will be a potluck, so please bring a dish to share. I will provide the main course, so you can focus on bringing a side dish, dessert, or appetizer.</p><p>Please RSVP by replying to this email or by calling me at 555-555-5555. I look forward to seeing you on Friday!</p><p>Yours,</p><p>Your Friendly Assistant</p></blockquote><br><h3 id=runtime-tests-2>Runtime tests</h3><table><thead><tr><th style=text-align:center>runtime / 50 tokens (sec)</th><th style=text-align:center>GPU</th><th style=text-align:center>attn</th><th style=text-align:center>torch dtype</th><th style=text-align:center>VRAM (GB)</th></tr></thead><tbody><tr><td style=text-align:center>0.61</td><td style=text-align:center>1x H100 (80 GB PCIe)</td><td style=text-align:center>triton</td><td style=text-align:center>bfloat16</td><td style=text-align:center>12</td></tr><tr><td style=text-align:center>0.67</td><td style=text-align:center>1x H100 (80 GB PCIe)</td><td style=text-align:center>torch</td><td style=text-align:center>bfloat16</td><td style=text-align:center>12</td></tr><tr><td style=text-align:center>1.17</td><td style=text-align:center>1x A100 (40 GB SXM)</td><td style=text-align:center>triton</td><td style=text-align:center>bfloat16</td><td style=text-align:center>13</td></tr><tr><td style=text-align:center>1.36</td><td style=text-align:center>1x A100 (40 GB SXM)</td><td style=text-align:center>torch</td><td style=text-align:center>bfloat16</td><td style=text-align:center>13</td></tr><tr><td style=text-align:center>2.25</td><td style=text-align:center>1x V100 (16 GB SXM)</td><td style=text-align:center>torch</td><td style=text-align:center>float16</td><td style=text-align:center>13</td></tr><tr><td style=text-align:center>3.75</td><td style=text-align:center>1x V100 (16 GB SXM)</td><td style=text-align:center>torch</td><td style=text-align:center>fp4</td><td style=text-align:center>4</td></tr><tr><td style=text-align:center>4.84</td><td style=text-align:center>1x Tesla T4 (15 GB)</td><td style=text-align:center>torch</td><td style=text-align:center>float16</td><td style=text-align:center>13</td></tr><tr><td style=text-align:center>8.77</td><td style=text-align:center>1x Tesla T4 (15 GB)</td><td style=text-align:center>torch</td><td style=text-align:center>fp4</td><td style=text-align:center>4</td></tr></tbody></table><p>The above runtime stats were generated from this <a href=https://github.com/daniel-furman/sft-demos/blob/main/inf_tests/runtimes_mpt_7b_dolphin.ipynb>notebook</a>.</p><br></div></div><html><body><div id=footer class=mb-5><hr><div class="container text-center"><a href=https://daniel-furman.github.io/><small>By Daniel Furman</small></a></div></div></body><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css integrity=sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js integrity=sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O crossorigin=anonymous></script></html></body></html>