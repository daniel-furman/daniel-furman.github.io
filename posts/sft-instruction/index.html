<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Supervised finetuning of instruction-following LLMs</title>
<meta name=description content="Daniel Furman's Portfolio and Blog"><meta name=author content="Daniel Furman"><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous><link rel=stylesheet href=/sass/researcher.min.css><link rel=icon type=image/ico href=https://daniel-furman.github.io/favicon.ico></head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><body><div class="container mt-5"><nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0"><a class="navbar-brand mx-0 mr-sm-auto" href=https://daniel-furman.github.io/>Daniel Furman</a><div class="navbar-nav flex-row flex-wrap justify-content-center"><a class="nav-item nav-link" href=/>About
</a><span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/posts>Posts
</a><span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/bookshelf>Bookshelf
</a><span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/resume>Resume
</a><span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/contact>Contact</a></div></nav></div><hr><div id=content><div class=container><h1 id=supervised-finetuning-of-instruction-following-large-language-models-llms>Supervised finetuning of instruction-following Large Language Models (LLMs)</h1><hr><p>This blog post corresponds to the README from <a href=https://github.com/daniel-furman/sft-demos>here</a>.</p><hr><p>This repo contains demos for supervised finetuning (sft) of large language models, like Meta&rsquo;s <a href=https://huggingface.co/meta-llama/Llama-2-7b-hf>llama-2</a>. In particular, we focus on tuning for short-form instruction following capabilities.</p><h2 id=table-of-contents>Table of contents</h2><ol><li><a href=https://github.com/daniel-furman/sft-demos#instruction-tuning-background>Background</a></li><li><a href=https://github.com/daniel-furman/sft-demos#favorites-from-this-repo>Finetuned models</a></li><li><a href=https://github.com/daniel-furman/sft-demos#basic-usage>Basic usage</a></li><li><a href=https://github.com/daniel-furman/sft-demos#base-models-and-datasets>Base models and datasets</a></li></ol><hr><h2 id=instruction-tuning-background>Instruction-tuning background</h2><p>The goal of instruction-tuning is to build LLMs that are capable of following natural language instructions to perform a wide range of tasks. The below was captured from the &ldquo;<a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">State of GPTs</a>&rdquo; talk by Andrej Karpathy. The key points illustrated for sft:</p><ul><li>Collect small but high-quality datasets in the form of prompt and ideal responses.</li><li>Do language modeling on this data, nothing changes algorithmically from pretraining.</li><li>After training we get an sft model which can be deployed as assistants (and it works to some extent).</li></ul><p><img src=https://raw.githubusercontent.com/daniel-furman/sft-demos/main/assets/assistant_training_pipeline.png alt=training_pipeline></p><p>For more background, see any number of excellent papers on the subject, including <a href=https://arxiv.org/pdf/2212.10560.pdf>Self-Instruct</a> (2023), <a href=https://arxiv.org/pdf/2306.02707.pdf>Orca</a> (2023), and <a href=https://arxiv.org/pdf/2203.02155.pdf>InstructGPT</a> (2022).</p><h2 id=favorites-from-this-repo>Favorites from this repo</h2><ol><li><a href=https://huggingface.co/dfurman/Llama-2-70B-Instruct-v0.1>dfurman/Llama-2-70B-Instruct-v0.1</a><ul><li><em>Note</em>: This model was ranked 6th on ðŸ¤—&rsquo;s Open LLM Leaderboard in Aug 2023</li></ul></li><li><a href=https://huggingface.co/dfurman/Mistral-7B-Instruct-v0.1>dfurman/Mistral-7B-Instruct-v0.1</a></li><li><a href=https://huggingface.co/dfurman/Falcon-180B-Instruct-v0.1>dfurman/Falcon-180B-Instruct-v0.1</a></li></ol><h2 id=basic-usage>Basic usage</h2><p><em>Note</em>: Use the code below to get started with the sft models herein, as ran on 1x A100.</p><p><strong>dfurman/Mistral-7B-Instruct-v0.1</strong></p><details><summary>Setup</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=o>-</span><span class=n>q</span> <span class=o>-</span><span class=n>U</span> <span class=n>transformers</span> <span class=n>peft</span> <span class=n>torch</span> <span class=n>accelerate</span> <span class=n>einops</span> <span class=n>sentencepiece</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>PeftModel</span><span class=p>,</span> <span class=n>PeftConfig</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>AutoModelForCausalLM</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>AutoTokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>peft_model_id</span> <span class=o>=</span> <span class=s2>&#34;dfurman/Mistral-7B-Instruct-v0.1&#34;</span>
</span></span><span class=line><span class=cl><span class=n>config</span> <span class=o>=</span> <span class=n>PeftConfig</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>peft_model_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>peft_model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>use_fast</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span><span class=o>.</span><span class=n>base_model_name_or_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>PeftModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>peft_model_id</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></details><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Tell me a recipe for a mai tai.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>*** Prompt:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>input_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span>
</span></span></code></pre></td></tr></table></div></div><details><summary>Prompt</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&lt;s&gt; [INST] Tell me a recipe for a mai tai. [/INST]&#34;</span>
</span></span></code></pre></td></tr></table></div></div></details><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>*** Generate:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>autocast</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>input_ids</span><span class=o>=</span><span class=n>input_ids</span><span class=o>.</span><span class=n>cuda</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>1024</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>temperature</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>return_dict_in_generate</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>eos_token_id</span><span class=o>=</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>pad_token_id</span><span class=o>=</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>repetition_penalty</span><span class=o>=</span><span class=mf>1.2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>no_repeat_ngram_size</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span><span class=p>[</span><span class=s2>&#34;sequences&#34;</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=nb>len</span><span class=p>(</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>]):],</span> 
</span></span><span class=line><span class=cl>    <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><details><summary>Generation</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;1 oz light rum
</span></span></span><span class=line><span class=cl><span class=s2>Â½ oz dark rum
</span></span></span><span class=line><span class=cl><span class=s2>Â¼ oz orange curaÃ§ao
</span></span></span><span class=line><span class=cl><span class=s2>2 oz pineapple juice
</span></span></span><span class=line><span class=cl><span class=s2>Â¾ oz lime juice
</span></span></span><span class=line><span class=cl><span class=s2>Dash of orgeat syrup (optional)
</span></span></span><span class=line><span class=cl><span class=s2>Splash of grenadine (for garnish, optional)
</span></span></span><span class=line><span class=cl><span class=s2>Lime wheel and cherry garnishes (optional)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>Shake all ingredients except the splash of grenadine in a cocktail shaker over ice. Strain into an old-fashioned glass filled with fresh ice cubes. Gently pour the splash of grenadine down the side of the glass so that it sinks to the bottom. Add garnishes as desired.&#34;&#34;&#34;</span>
</span></span></code></pre></td></tr></table></div></div></details><h2 id=base-models-and-datasets>Base models and datasets</h2><p>We finetune off of the following base models in this repo:</p><ul><li><a href=https://huggingface.co/01-ai>yi</a></li><li><a href=https://huggingface.co/mistralai/Mistral-7B-v0.1>mistral</a></li><li><a href=https://huggingface.co/meta-llama/Llama-2-70b-hf>llama-2</a></li><li><a href=https://huggingface.co/tiiuae/falcon-180B>falcon</a></li><li><a href=https://huggingface.co/mosaicml/mpt-7b>mpt</a></li></ul><p>We use the following datasets in this repo:</p><ul><li><a href=https://huggingface.co/datasets/ehartford/dolphin>ehartford/dolphin</a></li><li><a href=https://huggingface.co/datasets/jondurbin/airoboros-2.2.1>jondurbin/airoboros-2.2.1</a></li><li><a href=https://huggingface.co/datasets/garage-bAInd/Open-Platypus>garage-bAInd/Open-Platypus</a></li><li><a href=https://huggingface.co/datasets/timdettmers/openassistant-guanaco>timdettmers/openassistant-guanaco</a></li></ul><hr></div></div><html><body><div id=footer class=mb-5><hr><div class="container text-center"><a href=https://daniel-furman.github.io/><small>By Daniel Furman</small></a></div></div></body><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css integrity=sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js integrity=sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O crossorigin=anonymous></script></html></body></html>