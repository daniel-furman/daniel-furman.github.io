<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>BirdCLEF21 Kaggle Competition Write Up.</title><meta name=description content="Daniel Furman's Portfolio and Blog"><meta name=author content="Daniel Furman"><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous><link rel=stylesheet href=/sass/researcher.min.css><link rel=icon type=image/ico href=https://daniel-furman.github.io/favicon.ico></head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><body><div class="container mt-5"><nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0"><a class="navbar-brand mx-0 mr-sm-auto" href=https://daniel-furman.github.io/>Daniel Furman</a><div class="navbar-nav flex-row flex-wrap justify-content-center"><a class="nav-item nav-link" href=/>About</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/posts>Posts</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/coding>Coding</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/bookshelf>Bookshelf</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/contact>Contact</a></div></nav></div><hr><div id=content><div class=container><h2 id=birdclef21-kaggle-competition-write-up>BirdCLEF21 Kaggle Competition Write Up.</h2><hr><p>The BirdCLEF21 Kaggle challenge tasked competitors to classify bird calls by their species (397 target classes). Overall, bird call recognition models are aimed at streamlining (semi-automating) ecosystem monitoring of bird assemblages. The competition's focus on actionable outcomes that help the planet was particularly intriguing to me, and I thoroughly enjoyed the time I put into these efforts.</p><p><strong>TLDR</strong></p><p>For BirdCLEF21, I blended several CNNs taking 7-sec spectrogram representations of audio bird call files. I employed stripe augmentations and mixup to improve the CNNs’ generalization to out-of-training domains. For inference, I predict on 5-sec snippets padded to 7-sec and refine the result with a metadata gradient boosting classifier and postprocessing. The domain shift from the training set, composed of short bird call recordings (train_short) to the test-set, composed of passively recorded natural soundscapes, resulted in a significant drop off in performance upon inference to the hidden test set.</p><p>My solution achieved a top 9% result (solo bronze) and moved up 8 spots from the public LB (f1 = 0.68) to the private LB (f1 = 0.61).</p><p><strong>Validation</strong></p><p>There were 4 total sites in the hidden test-set, composed of passively recorded natural soundscapes that were approximately ten minutes long. Robust validation in this competition was difficult for a number of reasons. For example, the available sample of test data (training soudscapes) contained merely 35% of the total data, missed two of the four sites, and contained 3 full songs of only nocall. There was also significant domain shifts from the training data (train_short) to the testing data (passively recorded natural soundscapes). In addition, many of the species were underrepresented in the training-set (roughly 1/3 had less than 100 instances). Furthermore, the test-set likely contained species that were absent from the training set entirely (calling for <a href="https://www.youtube.com/watch?v=Ag1bw8MfHGQ">self-supervised learning networks</a> for more robust model deployment). I believe that self-supervised models capable of predicting out-of-training classes would generate more actionable biodiversity outcomes, the main goal of BirdCLEF, and I hope that the next iteration of the competition will include out-of-training species in the task (there are over <a href="https://www.amnh.org/about/press-center/new-study-doubles-the-estimate-of-bird-species-in-the-world#:~:text=Birds%20are%20traditionally%20thought%20of,and%2010%2C000%20species%20of%20birds.">~20k bird species</a>).</p><p>In an attempt to correct for auto spatial correlation in the survey records, I implemented spatial 5-fold cross validation using the block technique. Because folding is somewhat redundant relative to the true train/test domain shift, I selected the best performing models across different seeds and modeling parameters, resulting in a bagged model with each of the 5 folds represented at least once and at most twice. Despite the redundancies in folding the training set, my cross-validation scheme is preferred to random splitting in respect to spatial auto correlations common to ecological survey data. In addition, model inference in the competition and performance in real-world deployment would be hindered by randomly leaving out 20% of the data, which could easily contain types of calls underrepresented in the data.</p><p>The <a href=https://cran.r-project.org/web/packages/blockCV/vignettes/BlockCV_for_SDM.html>blockCV</a> strategy employed spatially split the data into grids, doing so for each quarter of the year. Due to the density of surveys within the training set, I considered three large areas for the gridding scheme (North America, South America, and Europe), regions which had a high density of recordings, and otherwise randomly assigned folds for regions with a low density of recordings. The geospatial gridding forced the models to learn how to extrapolate in space from training folds to the the validation fold. Furthermore, the temporal splitting into yearly quarters ensured the 80/20 train/test split was preserved across time periods, so that seasonal patterns in the bird calls were captured in a representative manner.</p><p>(Figure to come)</p><p><strong>Code and Data Pipeline</strong></p><p>I used <code>Google Drive</code> for code storage, <code>Neptune.ai</code> for experiment tracking, and <code>Google Colab</code> for model fitting in a high-RAM & GPU enabled environment. For the CNNs I used <code>PyTorch</code> with Resnest and EfficientNet backbones and for the Metadata classifier I used <code>Catboost</code>. Since I used <a href=https://www.kaggle.com/kneroma/kkiller-birdclef-2021>pre-computed mel-spectrograms</a> for the CNNs there was no need for data versioning (Github for future storage with versioning). Additionally, the spectrograms were cached to memory before training to speed up CNN fitting. I performed the CNN augmentations (mixup, striping) on the GPU per batch to reduce CPU bottlenecking.</p><p><strong>Bird Call CNNs</strong></p><p>My models were trained on the train_short clips and evaluated with the Public LB soundscapes. They were all trained on 7-sec crops of the train_short data, with up to ten spectrograms taken at the beginning of the files (35% of the clips had the max ten 7-sec crops). Taking longer 30 second clips would likely have been a better strategy (used by many of the top scorers), because longer snippets are superior among the weakly labeled data (unsure where the bird is calling). To account for the 5sec snippet format of test data, I padded the 5-sec clips at model inference to 7-sec.</p><p>Backbones: <a href=https://www.kaggle.com/ttahara/resnest50-fast-package>resnest50</a> (striping), <a href=https://www.kaggle.com/tunguz/efficientnet-pytorch-071>efficientnet-B3</a> (mixup). Transfer weights from ImageNet.</p><p>Model training in Colab for the resnests was ~2.5 hours (12 epochs), while training for the effnets was ~5.5 hours (55 epochs).</p><p>(Figure to come)</p><p>I trained with BCE loss using Adam optimizer and cosine annealing schedule. I saw improvements using the following tricks:</p><ul><li>Augmentations. Power transformation (random int between 0.5 and 3) so to randomly vary the contrast of the images for each batch. Striping vertically and horizontally across the images (for 80% of the samples). Mixup between images and updating the labels accordingly, using the standard FB Cifar10 params (for 50-100% of the samples, depending on the run).<br></li><li>Label smoothing. I used label smoothing to account for noisy annotations and absence of birds in “unlucky” 7sec crops. I used 0.0025 for zeros, 0.3 for secondary labels, and 0.995 for the primary label.</li><li>Next time: quality rating as weight on loss, with rating/max(ratings).</li><li>Next time: add background pink noise</li><li>Next time: consider other normalization methods for training.</li></ul><p><strong>Metadata Classifier</strong></p><p>In addition to the audio based neural networks, I also trained a gradient boosting learner based only on the metadata associated with each call. I incorporated nine features to predict the primary labels in the training set, leaving out secondary ones (all data used). The features included:</p><ul><li>3 BioClims pertinent for bird assemblages (<a href=https://www.nature.com/articles/s41598-019-53409-6>Bender et al., 2017</a>), forest type (categorical), grass cover, datetimes (e.g., 1-365), longitude, latitude, and elevation. (<a href=https://www.kaggle.com/dryanfurman/geospatialdatabirdclef>raw .tif files</a>).</li></ul><p>To extract these data, I used the raster surface value at the given location (long/lat coordinates associated with each bird call instance). The BioClim rasters had a 5-arcmin resolution, while the land cover features were aggregated to match the BioClims. The numerical features were then subtracted by their mean and divided by their std so to z-score normalize the features. The Catboost model took approximately 1 hour to complete 1208 iterations on the GPU, with use-best-iteration enabled, yielding a f1 score of 0.18 on the randomly selected 20% validation set.</p><p>(Figure to come)</p><p>For real-world deployment, an accurate incorporation of the ecology and biogeography would be essential for the product's stability over time (and domain shift). Postprocessing was then employed to further incorporate these trends, aimed at improving the incorporation of the geospatial constraints (see below).</p><p><strong>Ensembling</strong></p><p>I bagged the probabilistic inferences to blend the models into an ensemble (predict_proba). Un-weighted averaging was employed for the same type of model in the 5 folds. I then used weighted averaging upon blending multiple models (aka both the stripe and mixup augmentations, as well as the metadata classifier). I could have used hyperparam tuning with hyperopt, but instead I opted for common sense first-guess and then refined from there, based on the public LB. In the end, I blended the metadata classifier at a 0.14 weight relative to the CNNs (with weight 1).</p><p><strong>Postprocessing</strong></p><p>I then used a threshold to generate the labels from the raw probabilistic inferences. If any label had a prediction above the threshold, then that species was included in the final prediction (multiple label predictions were thus possible). If no labels received a prediction greater than the threshold, it received the nocall label. I used the given training soundscapes as the toy test set for tuning the threshold against the public LB performance. I did this by calculating f1 scores for different thresholds, and plotted the results in matplotlib. I then randomly tested thresholds on either side of the maximum.</p><ul><li>Next time: Bootstrapping of validation set</li><li>Next time: Geospatial limitations</li><li>Next time: Dynamic thresholding</li></ul><p>Thanks goes to LifeCLEF, Kaggle, and the competitors. Cites: kkiller, Jan’s paper, 2nd place write up, 11 place write up.</p></div></div><html><body><div id=footer class=mb-5><hr><div class="container text-center"><a href=https://daniel-furman.github.io/><small>By Daniel Furman</small></a></div></div></body><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css integrity=sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js integrity=sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O crossorigin=anonymous></script></html></body></html>