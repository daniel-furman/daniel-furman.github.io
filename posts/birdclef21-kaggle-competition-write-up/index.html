<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>BirdCLEF21 Kaggle Competition Write Up.</title><meta name=description content="Daniel Furman's Portfolio and Blog"><meta name=author content="Daniel Furman"><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous><link rel=stylesheet href=/sass/researcher.min.css><link rel=icon type=image/ico href=https://daniel-furman.github.io/favicon.ico></head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><body><div class="container mt-5"><nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0"><a class="navbar-brand mx-0 mr-sm-auto" href=https://daniel-furman.github.io/>Daniel Furman</a><div class="navbar-nav flex-row flex-wrap justify-content-center"><a class="nav-item nav-link" href=/>About</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/posts>Posts</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/coding>Coding</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/bookshelf>Bookshelf</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/contact>Contact</a></div></nav></div><hr><div id=content><div class=container><h2 id=birdclef21-kaggle-competition-write-up>BirdCLEF21 Kaggle Competition Write Up.</h2><hr><p>The BirdCLEF21 Kaggle competition task was to classify bird calls by their species (397 species). The bird call recognition models were aimed at streamlining/semi-automating ecosystem monitoring for bird assemblages.</p><p><strong>TLDR</strong></p><p>I blended several CNNs taking 7-second spectrogram representations of .ogg bird call files. I employed stripe augmentations and mixup to improve the CNNs’ generalization to out of training domains. For inference, we predict on padded 5 sec snippets and refine the result with a metadata gradient boosting classifier and postprocessing to account for underlying ecological constraints at play.</p><p>The domain shift from the training set, composed of short bird call recordings (train_short) to the test-set, composed of passively recorded natural soundscapes, resulted in a significant drop off in model performance at inference to the hidden test set.</p><p>My solution achieved a top 9% result (solo bronze) and moved up 8 spots from the public LB (f1 = 0.68) to the private LB (f1 = 0.61).</p><p><strong>Validation</strong></p><p>There were 4 total sites in the hidden test-set, composed of passively recorded natural soundscapes that are approximately ten minutes long. Robust validation in this competition was difficult for a number of reasons. Firstly, the available sample of test data contained merely 35% of the total data, missed two of the four sites, and contained 3 full songs of nocalls. Secondly, there was a significant domain shift from the training data of short bird call audio files to the testing data of passively recorded natural soundscapes. Thirdly, many of the bird species were underrepresented in the training-set (roughly 1/3 had less than 100 instances), and the test-set likely contained species that were absent from the training set (possibly calling for self-supervised networks for better deployment performance?). I definitely believe self-supervised models capable of predicting out-of-training classes would generate more actionable biodiversity outcomes for model deployment (the main goal of BirdCLEF21 and LifeCLEF).</p><p>In an attempt to correct for auto spatial correlation in the survey records, I implemented spatial 5-fold cross validation using the block technique. Because folding is somewhat redundant relative to the true train/test domain shift, I selected the best performing models across different seeds and modeling parameters, resulting in a bagged model with each of the 5 folds represented at least once and at most twice. Despite the redundancies in folding the training set, my cross-validation scheme is preferred to random splitting in respect to spatial auto-correlations common to ecological survey data. In addition, model inference in the competition and performances at real-world deployment would be hindered by randomly leaving out 20% of the calls, which could easily contain types of calls that were underrepresented in the data.</p><p>The block cross validation strategy I employed split the data into geospatial grids, doing so across temporal splits for each quarter of the year. Due to the density of surveys in the training set, I considered three larger areas for the gridding scheme (North America, South America, and Europe), regions which had a high density of recordings, and otherwise randomly assigned folds for regions with a low density of recordings. The geospatial gridding caused an extrapolation in space from the training folds to the test set aimed at improving the models’ extrapolation to novel data, while the temporal splitting into yearly quarters ensured the 80/20 train/test split was preserved across time periods, so that seasonal patterns in the bird calls were captured in a representative manner.</p><p>(Figure to come)</p><p><strong>Code and Data Pipeline</strong></p><p>I used Google Drive for code storage, Neptune.ai for experiment tracking, and Google Colab for model fitting in a high-RAM & GPU enabled environment. For the CNNs I used PyTorch with Resnest and EfficientNet backbones and for the Metadata classifier I used Catboost. Since I used Kkiller’s pre-computed spectrograms as my data source for the CNNs there was no need for data versioning (I would have used Github for storage if so). Additionally, the spectrograms were cached to memory before training to speed up the pipeline, and I performed the CNN augmentations (mixup, striping) on the GPU to reduce CPU bottlenecking.</p><p><strong>Bird Call CNNs</strong></p><p>My models were trained on the train_short clips and evaluated with the given Public LB soundscapes. They were all trained on 7-sec crops of the train_short data, with up to ten spectrograms taken at the beginning of the files (x% of the clips had the max ten). Taking 30 second clips would likely have been a better strategy (used by many of the top scorers), because longer snippets are superior among the weakly labeled data (unsure where the bird is calling). To account for the 5sec snippet format of test data, I padded the 5-sec clips at model inference to 7-sec.</p><p>Backbones: resnest50 (striping), efficientnet-B3 (mixup)</p><p>Model training in Colab for the resnests was ~2.5 hours (12 epochs), while training for the effnets was ~5.5 hours (55 epochs).</p><p>(Figure to come)</p><p>I trained with BCE loss using Adam optimizer and cosine annealing schedule. I saw improvements using the following tricks:</p><p>Augmentations. Power transformation (random int between 0.5 and 3) so to randomly vary the contrast of the images for each batch. Striping vertically and horizontally across the images (for 80% of the samples). Mixup between images and updating the labels accordingly, using the standard FB Cifar10 params (for 50-100% of the samples, depending on the run).<br>Label smoothing. I used label smoothing to account for noisy annotations and absence of birds in “unlucky” 7sec crops. I used 0.0025 for zeros, 0.3 for secondary labels, and 0.995 for the primary label.
Next time: record rating as weight on loss, with rating/max(ratings).
Next time: add background pink noise</p><p><strong>Metadata Classifier</strong></p><p>In addition to the audio based neural networks, I also trained a gradient boosting learner based only on the metadata associated with each call. Ideally, this gradient booster will add a new flavor to the ensemble, picking out certain trends that the raw audio CNNs might miss.</p><p>I incorporated nine metadata features to predict the primary labels in the training set, leaving out secondary ones (all data used, 397 classes). The features included three BioClim climatic variables proven to be important to bird assemblages (site), forest type (categorical), grass cover, datetimes (e.g., 1-365), longitude, latitude, and elevation. To extract these features I employed the given coordinates to back out the values on the features’ raster surfaces. The BioClim rasters had a 5-arcmin resolution, while the land cover features were aggregated to match the BioClims. The numerical features were then subtracted by their mean and divided by their std so to z-score normalize the features. The Catboost model took approximately 1 hour to complete 1208 iterations on the GPU, with use-best-iteration enabled, yielding a f1 score of 0.18 on the randomly selected 20% validation set.</p><p>(Figure to come)</p><p>For model deployment in the real-world for actionable ecological outcomes, a robust incorporation of the pertinent ecological and biogeographic constraints is essential for model stability/performance over time (and domain shift). Postprocessing was then employed to further incorporate these trends, aimed at improving the incorporation of the geospatial constraints (see below).</p><p><strong>Ensembling</strong></p><p>I used bagging on the probabilistic inferences. Averaging was employed for the same type of model in the 5 folds. I then used weighted averaging upon blending multiple models (aka both the stripe and mixup augmentations, as well as the metadata classifier). I could have used hyperparam tuning with hyperopt, but instead I opted for common sense first guess and then refinement from there, as based on the public LB. In the end, I blended the metadata classifier at a 0.14 weight relative to the CNNs (with weight 1).</p><p><strong>Postprocessing</strong></p><p>I then used a threshold to go from the probabilistic inferences to the final labels. If any label had a prediction above the threshold, then that species was included in the final prediction (multiple label predictions were thus possible). If no labels received a prediction greater than the threshold, it received the nocall label. I used the given “training soundscapes” as the toy test set for tuning the threshold (as well as the public LB). I did this by calculating f1 scores for different thresholds, and plotted the results in matplotlib. I then played around with thresholds near the optimum, recording the public LB stats as they were generated.</p><p>Next time: Bootstrapping of validation set
Next time: Geospatial limitations
Next time: Dynamic thresholding</p><p>Thanks goes to LifeCLEF, Kaggle, and the competitors. Cites: kkiller, jan’s paper, 2nd place write up, 11 place write up.</p></div></div><html><body><div id=footer class=mb-5><hr><div class="container text-center"><a href=https://daniel-furman.github.io/><small>By Daniel Furman</small></a></div></div></body><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css integrity=sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js integrity=sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O crossorigin=anonymous></script></html></body></html>