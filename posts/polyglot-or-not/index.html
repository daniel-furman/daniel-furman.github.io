<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Are foundation language models polyglots?</title><meta name=description content="Daniel Furman's Portfolio and Blog"><meta name=author content="Daniel Furman"><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous><link rel=stylesheet href=/sass/researcher.min.css><link rel=icon type=image/ico href=https://daniel-furman.github.io/favicon.ico></head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><body><div class="container mt-5"><nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0"><a class="navbar-brand mx-0 mr-sm-auto" href=https://daniel-furman.github.io/>Daniel Furman</a><div class="navbar-nav flex-row flex-wrap justify-content-center"><a class="nav-item nav-link" href=/>About</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/posts>Posts</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/bookshelf>Bookshelf</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/resume>Resume</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/contact>Contact</a></div></nav></div><hr><div id=content><div class=container><h1 id=polyglot-or-not-measuring-multilingual-encyclopedic-knowledge-retrieval-from-foundation-language-models>Polyglot or Not?: Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models</h1><hr><p>Post corresponds to the README from this <a href=https://github.com/daniel-furman/polyglot-or-not>repo</a>.</p><hr><p>This is the repository for the following paper: <a href=https://arxiv.org/abs/2305.13675>Polyglot or Not?: Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models</a>. It contains several research artifacts, including:</p><ol><li>The <a href=https://github.com/daniel-furman/Polyglot-or-Not/blob/main/notebooks/fact_completion_notebooks/fact-completion-full-benchmark.ipynb>code</a> for running the fact-completion test</li><li>Our <a href=https://huggingface.co/datasets/Polyglot-or-Not/Fact-Completion>dataset</a> of factual associations translated into 20 languages</li><li>A <a href=https://github.com/daniel-furman/Polyglot-or-Not/blob/main/notebooks/fact_completion_notebooks/fact-completion-lightweight-demo.ipynb>demo</a> of contrastive knowledge assessment</li></ol><h2 id=method>Method</h2><p>Given a factual association such as <em>The capital of France is <strong>Paris</strong></em>, we determine whether a model adequately &ldquo;knows&rdquo; the correct completion with the following test:</p><ul><li><strong>Step 1</strong>: prompt the model to predict the likelihood of the token <strong>Paris</strong> following <em>The Capital of France is</em></li><li><strong>Step 2</strong>: prompt the model to predict the average likelihood of a set of false, counterfactual tokens following the same stem.</li></ul><p>If the value from <strong>Step 1</strong> is greater than the value from <strong>Step 2</strong> we conclude that the model adequately recalls that fact. Formally, this is an application of the <em>Contrastive Knowledge Assessment</em> proposed in [<a href=https://github.com/daniel-furman/Polyglot-or-Not#bibliography>1</a>].</p><h2 id=models-evaluated>Models Evaluated</h2><p>We evaluate 5 foundation models of interest in a multilingual setting, like <a href=https://arxiv.org/abs/2302.13971>Llama</a> [<a href=https://github.com/daniel-furman/Polyglot-or-Not#bibliography>2</a>]. We perform this assessment with 303k fact-completions spanning 20 languages (<a href=https://github.com/daniel-furman/Polyglot-or-Not#test-results>results</a>).</p><p>In addition to our multilingual assessment, we also scored a diverse set of over 25 models (like <a href=https://huggingface.co/meta-llama>Llama-2</a>, <a href=https://huggingface.co/tiiuae/falcon-40b>Falcon</a>, <a href=https://arxiv.org/abs/2204.06745>GPT-NeoX</a>, and <a href=https://arxiv.org/abs/2205.01068>OPT</a>) on the English-only subset of our dataset, which comprises 26.3k fact-completions.</p><p>While we would have liked to test close-sourced models, such as OpenAI&rsquo;s GPT-4, such models do not provide vocabulary-wide probabilities at inference. These models are thus incompatible at present with our contrastive knowledge assessment test. As such, our study demonstrates the need for all LLMs - open and closed - to produce vocabulary-wide probabilities for more robust evaluations.</p><h2 id=data-release>Data Release</h2><p>We present 303k unique fact-completions in <a href=https://huggingface.co/datasets/Polyglot-or-Not/Fact-Completion><code>Polyglot-or-Not/Fact-Completion</code></a>, which are in the form of {stem, fact, counterfact} triples. See the <a href=https://huggingface.co/datasets/Polyglot-or-Not/Fact-Completion/viewer/Polyglot-or-Not--Fact-Completion/English>dataset viewer</a> for a closer look.</p><ul><li>20 Latin/Cyrillic script languages are included. The ISO 639-1 language codes are: <code>bg</code>, <code>ca</code>, <code>cs</code>, <code>da</code>, <code>de</code>, <code>en</code>, <code>es</code>, <code>fr</code>, <code>hr</code>, <code>hu</code>, <code>it</code>, <code>nl</code>, <code>pl</code>, <code>pt</code>, <code>ro</code>, <code>ru</code>, <code>sl</code>, <code>sr</code>, <code>sv</code>, and <code>uk</code>.</li></ul><p>The factual associations were originally sourced from English-language Wikidata curated in the T-REx dataset [<a href=https://github.com/daniel-furman/Polyglot-or-Not#bibliography>3</a>] as utilized in factual association research such as [<a href=https://github.com/daniel-furman/Polyglot-or-Not#bibliography>1</a>] and [<a href=https://github.com/daniel-furman/Polyglot-or-Not#bibliography>4</a>]. We used the Google Translate API alongside bespoke wrapper <a href=https://github.com/daniel-furman/Polyglot-or-Not/blob/main/src/dataset_caching_scripts/language_translation_helper.py>code</a> to programmatically generate the non-English cuts.</p><h2 id=test-results>Test Results</h2><h3 id=multilingual-leaderboard><strong>Multilingual</strong> leaderboard</h3><table><thead><tr><th>model</th><th style=text-align:center>accuracy (%)</th><th style=text-align:center>params</th><th style=text-align:center><em>n</em> tokens</th></tr></thead><tbody><tr><td><a href=https://huggingface.co/docs/transformers/main/model_doc/llama#llama>llama-33b</a></td><td style=text-align:center><strong>79.31</strong> (+/- 0.74)</td><td style=text-align:center>32.5B</td><td style=text-align:center>1.4T</td></tr><tr><td><a href=https://huggingface.co/bert-base-multilingual-cased>m-bert</a></td><td style=text-align:center><strong>62.00</strong> (+/- 0.87)</td><td style=text-align:center>110M</td><td style=text-align:center>-</td></tr><tr><td><a href=https://huggingface.co/bigscience/bloom-7b1>bloom-7b1</a></td><td style=text-align:center><strong>57.70</strong> (+/- 0.88)</td><td style=text-align:center>7.1B</td><td style=text-align:center>341B</td></tr><tr><td><a href=https://huggingface.co/xlm-roberta-large>xlm-roberta</a></td><td style=text-align:center><strong>56.03</strong> (+/- 0.90)</td><td style=text-align:center>355M</td><td style=text-align:center>295B</td></tr><tr><td><a href=https://huggingface.co/google/mt5-xl>mt5-xl</a></td><td style=text-align:center><strong>52.51</strong> (+/- 0.91)</td><td style=text-align:center>3.7B</td><td style=text-align:center>-</td></tr><tr><td>Random Baseline</td><td style=text-align:center>50</td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr></tbody></table><p><strong>Table 1</strong>: Multilingual test leaderboard. Here, <strong>accuracy</strong> refers to the average performance of each model across 20 distinct languages. The uncertainty estimates represent averaged 95% confidence intervals computed from 10000 bootstrap iterations per language. <strong>Params</strong> and <strong><em>n</em> tokens</strong> record each model’s number of parameters and number of dataset tokens, respectively (when such data is available). These results reveal that models struggle to recall facts in a multilingual setting, as compared to their English-only performance (Table <a href=https://github.com/daniel-furman/Polyglot-or-Not#english-only-leaderboard>2</a>). For instance, on average, Llama-33B&rsquo;s accuracy decreased by approximately 11% from English to non-English languages.</p><p> </p><h3 id=english-only-leaderboard><strong>English-only</strong> leaderboard</h3><table><thead><tr><th>model</th><th style=text-align:center>accuracy (%)</th><th style=text-align:center>params</th><th style=text-align:center><em>n</em> tokens</th></tr></thead><tbody><tr><td><a href=https://huggingface.co/meta-llama>llama-2-70b</a></td><td style=text-align:center><strong>90.86</strong> (+/- 0.35)</td><td style=text-align:center>70B</td><td style=text-align:center>2T</td></tr><tr><td><a href=https://huggingface.co/docs/transformers/main/model_doc/llama#llama>llama-65b</a></td><td style=text-align:center><strong>89.56</strong> (+/- 0.37)</td><td style=text-align:center>65.2B</td><td style=text-align:center>1.4T</td></tr><tr><td><a href=https://huggingface.co/docs/transformers/main/model_doc/llama#llama>llama-33b</a></td><td style=text-align:center><strong>89.40</strong> (+/- 0.38)</td><td style=text-align:center>32.5B</td><td style=text-align:center>1.4T</td></tr><tr><td><a href=https://huggingface.co/meta-llama>llama-2-13b</a></td><td style=text-align:center><strong>87.51</strong> (+/- 0.40)</td><td style=text-align:center>13B</td><td style=text-align:center>2T</td></tr><tr><td><a href=https://huggingface.co/tiiuae/falcon-40b>falcon-40b</a></td><td style=text-align:center><strong>87.01</strong> (+/- 0.41)</td><td style=text-align:center>40B</td><td style=text-align:center>1T</td></tr><tr><td><a href=https://huggingface.co/docs/transformers/main/model_doc/llama#llama>llama-13b</a></td><td style=text-align:center><strong>86.66</strong> (+/- 0.42)</td><td style=text-align:center>12.5B</td><td style=text-align:center>1T</td></tr><tr><td><a href=https://huggingface.co/meta-llama>llama-2-7b</a></td><td style=text-align:center><strong>86.22</strong> (+/- 0.42)</td><td style=text-align:center>7B</td><td style=text-align:center>2T</td></tr><tr><td><a href=https://huggingface.co/docs/transformers/main/model_doc/llama#llama>llama-7b</a></td><td style=text-align:center><strong>85.53</strong> (+/- 0.43)</td><td style=text-align:center>6.7B</td><td style=text-align:center>1T</td></tr><tr><td><a href=https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1>redpajama-7b</a></td><td style=text-align:center><strong>85.07</strong> (+/- 0.44)</td><td style=text-align:center>7B</td><td style=text-align:center>800B</td></tr><tr><td><a href=https://huggingface.co/mosaicml/mpt-7b>mpt-7b</a></td><td style=text-align:center><strong>83.39</strong> (+/- 0.46)</td><td style=text-align:center>7B</td><td style=text-align:center>1T</td></tr><tr><td><a href=https://huggingface.co/facebook/opt-30b>opt-30b</a></td><td style=text-align:center><strong>82.09</strong> (+/- 0.47)</td><td style=text-align:center>30B</td><td style=text-align:center>180B</td></tr><tr><td><a href=https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1>redpajama-3b</a></td><td style=text-align:center><strong>82.09</strong> (+/- 0.47)</td><td style=text-align:center>3B</td><td style=text-align:center>800B</td></tr><tr><td><a href=https://huggingface.co/facebook/opt-13b>opt-13b</a></td><td style=text-align:center><strong>81.94</strong> (+/- 0.46)</td><td style=text-align:center>13B</td><td style=text-align:center>30B</td></tr><tr><td><a href=https://huggingface.co/EleutherAI/gpt-neox-20b>gpt-neox-20b</a></td><td style=text-align:center><strong>81.50</strong> (+/- 0.47)</td><td style=text-align:center>20B</td><td style=text-align:center>420B</td></tr><tr><td><a href=https://huggingface.co/tiiuae/falcon-40b>falcon-7b</a></td><td style=text-align:center><strong>81.34</strong> (+/- 0.47)</td><td style=text-align:center>7B</td><td style=text-align:center>1.5T</td></tr><tr><td><a href=https://huggingface.co/EleutherAI/gpt-j-6b>gpt-j-6b</a></td><td style=text-align:center><strong>81.14</strong> (+/- 0.47)</td><td style=text-align:center>6B</td><td style=text-align:center>420B</td></tr><tr><td><a href=https://huggingface.co/EleutherAI/pythia-12b>pythia-12b</a></td><td style=text-align:center><strong>80.53</strong> (+/- 0.48)</td><td style=text-align:center>12B</td><td style=text-align:center>420B</td></tr><tr><td><a href=https://huggingface.co/google/t5-v1_1-xxl>t5-v1-xxl</a></td><td style=text-align:center><strong>76.55</strong> (+/- 0.52)</td><td style=text-align:center>13B</td><td style=text-align:center>34B</td></tr><tr><td><a href=https://huggingface.co/bigscience/bloom-7b1>bloom-7b1</a></td><td style=text-align:center><strong>76.16</strong> (+/- 0.51)</td><td style=text-align:center>7.1B</td><td style=text-align:center>341B</td></tr><tr><td><a href=https://huggingface.co/gpt2-xl>gpt2-xl</a></td><td style=text-align:center><strong>73.76</strong> (+/- 0.54)</td><td style=text-align:center>1.5B</td><td style=text-align:center>-</td></tr><tr><td><a href=https://huggingface.co/bert-base-uncased>bert</a></td><td style=text-align:center><strong>72.60</strong> (+/- 0.54)</td><td style=text-align:center>110M</td><td style=text-align:center>-</td></tr><tr><td><a href=https://huggingface.co/bert-base-multilingual-cased>m-bert</a></td><td style=text-align:center><strong>71.80</strong> (+/- 0.55)</td><td style=text-align:center>110M</td><td style=text-align:center>-</td></tr><tr><td><a href=https://huggingface.co/stabilityai/stablelm-base-alpha-7b>stablelm-7b</a></td><td style=text-align:center><strong>68.85</strong> (+/- 0.55)</td><td style=text-align:center>7B</td><td style=text-align:center>1.5T</td></tr><tr><td><a href=https://huggingface.co/distilgpt2>distilgpt2</a></td><td style=text-align:center><strong>64.23</strong> (+/- 0.59)</td><td style=text-align:center>82M</td><td style=text-align:center>-</td></tr><tr><td><a href=https://huggingface.co/google/mt5-xxl>mt5-xxl</a></td><td style=text-align:center><strong>61.58</strong> (+/- 0.59)</td><td style=text-align:center>13B</td><td style=text-align:center>-</td></tr><tr><td><a href=https://huggingface.co/xlm-roberta-large>xlm-roberta</a></td><td style=text-align:center><strong>61.55</strong> (+/- 0.59)</td><td style=text-align:center>355M</td><td style=text-align:center>295B</td></tr><tr><td><a href=https://huggingface.co/google/mt5-xl>mt5-xl</a></td><td style=text-align:center><strong>59.96</strong> (+/- 0.59)</td><td style=text-align:center>3.7B</td><td style=text-align:center>-</td></tr><tr><td>Random Baseline</td><td style=text-align:center>50</td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr></tbody></table><p><strong>Table 2</strong>: Monolingual test leaderboard. <strong>Accuracy</strong> represents performance on English-only data. The uncertainty estimates are 95% confidence intervals computed from 10000 bootstrap iterations. <strong>Params</strong> and <strong><em>n</em> tokens</strong> record each model’s number of parameters and number of dataset tokens, respectively (when such data is available). Consistent with the trends in Table <a href=https://github.com/daniel-furman/Polyglot-or-Not#multilingual-leaderboard>1</a>, Llamas of varying sizes emerge as the front-runners.</p><p> </p><h3 id=llama-33b-performance-across-languages><strong>Llama-33B</strong> performance across languages</h3><p><img src=/posts/LLaMa_h_bar_plot_final.png alt="Llama test leaderboard"></p><p><strong>Figure 1</strong>: Llama-33B&rsquo;s test performance across languages. Accuracy denotes the model&rsquo;s performance assessed individually for each language. The Llama-33B model demonstrates higher proficiency with languages utilizing the Latin script as compared to those using the Cyrillic script (Ukrainian, Bulgarian, Russian, and Serbian). A <a href=https://github.com/daniel-furman/Polyglot-or-Not/blob/main/notebooks/error_analysis/EntitySigTesting.ipynb>chi-squared test</a> substantiates a significant dependency of the model&rsquo;s test performance on the language script (<em>χ2</em> = 3570.576, <em>p</em> &lt; 0.001).</p><h2 id=authors>Authors</h2><ul><li>Daniel Furman <a href=mailto:daniel_furman@berkeley.edu>daniel_furman@berkeley.edu</a></li><li>Tim Schott <a href=mailto:timschott@berkeley.edu>timschott@berkeley.edu</a></li><li>Shreshta Bhat <a href=mailto:bhat_shreshta@berkeley.edu>bhat_shreshta@berkeley.edu</a></li></ul><h2 id=advisor>Advisor</h2><ul><li>David Bamman <a href=mailto:dbamman@berkeley.edu>dbamman@berkeley.edu</a></li></ul><h2 id=citation>Citation</h2><p>Please cite this repository as follows if you use its data or code:</p><pre tabindex=0><code>@misc{schott2023polyglot,
      doi = {10.48550/arXiv.2305.13675},
      title={Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models}, 
      author={Tim Schott and Daniel Furman and Shreshta Bhat},
      year={2023},
      eprint={2305.13675,
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
</code></pre><h2 id=bibliography>Bibliography</h2><p>[1] Calibrating Factual Knowledge in Pretrained Language Models. Dong, Qingxiu, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. In Findings of the Association for Computational Linguistics: EMNLP 2022. <a href=https://arxiv.org/abs/2210.03329>arXiv:2210.03329</a> (2022).</p><p>[2] Llama: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. <a href=https://arxiv.org/abs/2302.13971v1>https://arxiv.org/abs/2302.13971v1</a> (2023).</p><ul><li>Llama weights were accessed with the approval of Meta AI and used in accordance with the License (see <a href=https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform>link</a> for more details).</li></ul><p>[3] T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples. ElSahar, Hady, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest and Elena Paslaru Bontas Simperl. International Conference on Language Resources and Evaluation. <a href=http://aclanthology.lst.uni-saarland.de/L18-1544.pdf>Link</a> (2018).</p><p>[4] Mass Editing Memory in a Transformer. Meng, Kevin, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. arXiv preprint <a href=https://arxiv.org/abs/2210.07229>arXiv:2210.07229</a> (2022).</p></div></div><html><body><div id=footer class=mb-5><hr><div class="container text-center"><a href=https://daniel-furman.github.io/><small>By Daniel Furman</small></a></div></div></body><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css integrity=sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js integrity=sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O crossorigin=anonymous></script></html></body></html>