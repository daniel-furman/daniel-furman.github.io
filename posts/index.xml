<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Daniel Furman</title><link>https://daniel-furman.github.io/posts/</link><description>Recent content in Posts on Daniel Furman</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 04 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://daniel-furman.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Text clustering: HDBSCAN is probably all you need</title><link>https://daniel-furman.github.io/posts/hdbscan-is-all-you-need/</link><pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate><guid>https://daniel-furman.github.io/posts/hdbscan-is-all-you-need/</guid><description>Text clustering: HDBSCAN is probably all you need Post corresponds to the README from this repo.
Goal Segment common items in a text dataset to pinpoint core themes and their distribution.
Clusters cover the main topics/subtopics in the dataset Clusters backed by gpt-3.5-turbo-16k generated summaries Background We employ HDBSCAN for probabilistic clustering. This algorithm is advantageous in many ways, including:
Don’t be wrong: Cluster can have varying densities, don’t need to be globular, and won’t include noise Intuitive parameters: Choosing a minimum cluster size is very reasonable, and the number of k clusters does not need to be specified (HDBSCAN finds the optimal k for you) Stability: HDBSCAN is stable over runs and subsampling and has good stability over parameter choices Performance: When implemented well HDBSCAN can be very efficient; the current implementation has similar performance to fastcluster’s agglomerative clustering Citations Datasets fka/awesome-chatgpt-prompts gustavosta/stable-diffusion-prompts Embedding models sentence-transformers/all-mpnet-base-v2 Experiments 1.</description></item><item><title>Supervised finetuning of instruction-following LLMs</title><link>https://daniel-furman.github.io/posts/sft-instruction/</link><pubDate>Sat, 15 Jul 2023 00:00:00 +0000</pubDate><guid>https://daniel-furman.github.io/posts/sft-instruction/</guid><description>Supervised finetuning of instruction LLMs Post corresponds to the README from this repo.
This repo contains demos for supervised finetuning (sft) of large language models, like MosaicML&amp;rsquo;s mpt and Meta&amp;rsquo;s llama-2. In particular, we focus on short-form instruction following.
Instruction tuning background In recent years, there has been a growing interest in building models that can follow natural language instructions to perform a wide range of tasks. These models, known as &amp;ldquo;instruction-tuned&amp;rdquo; language models, have demonstrated the ability to generalize to new tasks.</description></item><item><title>Are foundation language models polyglots?</title><link>https://daniel-furman.github.io/posts/polyglot-or-not/</link><pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate><guid>https://daniel-furman.github.io/posts/polyglot-or-not/</guid><description>Polyglot or Not?: Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models Post corresponds to the README from this repo.
This is the repository for the following paper: Polyglot or Not?: Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models. It contains several research artifacts, including:
The code for running the fact-completion test Our dataset of factual associations translated into 20 languages A demo of contrastive knowledge assessment Method Given a factual association such as The capital of France is Paris, we determine whether a model adequately &amp;ldquo;knows&amp;rdquo; the correct completion with the following test:</description></item><item><title>Deep reinforcement learning for hyperspectral band selection</title><link>https://daniel-furman.github.io/posts/hyperspectraldrl/</link><pubDate>Sat, 17 Dec 2022 00:00:00 +0000</pubDate><guid>https://daniel-furman.github.io/posts/hyperspectraldrl/</guid><description>HyperSpectralDRL Intro The project aims at exploring different Deep Reinforcement Learning (DRL) algorithms for hyper-spectral band selection. Overall, we find that for this type of task Actor-Critic methods such as Actor-Critic and Soft Actor-Critic outperform traditional Q-Learning methods such as DQN and DDQN in terms of correlation and in many classification tasks. While we found that the agent is able to learn a valid representation of the search space, the effort and complexity of DRL (and most of the other published methods) may not be required for this problem.</description></item><item><title>Hugging Face models are all you need for CV classification</title><link>https://daniel-furman.github.io/posts/cv-eng-proj/</link><pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate><guid>https://daniel-furman.github.io/posts/cv-eng-proj/</guid><description>Exploring pre-trained image models on the Hugging Face hub as feature extractors and transfer learners in a land cover classification task Published in Towards Data Science</description></item><item><title>Why startup fundamentals are key to AI strategy</title><link>https://daniel-furman.github.io/posts/zero-to-one-ai/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://daniel-furman.github.io/posts/zero-to-one-ai/</guid><description>Why startup fundamentals are key to AI strategy Using the “technology startup mentality” to integrate technology in commercial contexts Published in Towards Data Science In his book Zero to One, Peter Thiel explores the core fundamentals that successful technology startups have in common. The takeaway: startups should embrace seven pivotal traits — otherwise, queue red flags. Here’s why great AI innovation projects also exhibit the fundamentals of successful technology startups, from the perspective of a Bay Area data scientist working in digital strategy consulting.</description></item><item><title>Field experiment on dating attitudes towards academic prestige</title><link>https://daniel-furman.github.io/posts/tinder-field-experiment/</link><pubDate>Mon, 09 May 2022 00:00:00 +0000</pubDate><guid>https://daniel-furman.github.io/posts/tinder-field-experiment/</guid><description>Field experiment on dating attitudes towards academic prestige in the Bay Area. By Daniel Furman, Forrest Brandt, and Jackie Hu Berkeley Spring 2022, Causal Inference with Prof. Josue Martinez Objective How significant is one’s academic prestige for online dating in the Bay Area? Are dating attitudes towards academic achievements different between men and women in the Bay Area?
The widespread adoption of online dating apps enables social scientists to perform field experiments probing the underlying psychology of human dating, for example, how social attractiveness influences dating dynamics.</description></item><item><title>DAT/Artathon risk forecasts for keystone California species</title><link>https://daniel-furman.github.io/posts/datartathon/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://daniel-furman.github.io/posts/datartathon/</guid><description>DAT/Artathon risk forecasts for keystone California species As featured in D/A&amp;rsquo;s gallery: https://datartathon.com/projects/2021-daniel-ecorisk-california
Made with the support of the Risk &amp;amp; Resilience DAT/Artathon 2021.
“EcoRisk California” explores climate change impacts for three of the West Coast’s most beloved tree species during the 21st century. The motion-graphics storyboard visualizes shifts within three species distributions across spatial and climate dimensions, exploring relationships between ecological risk and habitat suitability. To facilitate future experiments, the study is accompanied by an open-sourced Python package containing the class for semi-automated machine learning used in the visualization.</description></item><item><title>Using numerical mathematics to approximate the golden ratio</title><link>https://daniel-furman.github.io/posts/approximating-the-golden-ratio/</link><pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate><guid>https://daniel-furman.github.io/posts/approximating-the-golden-ratio/</guid><description>Using numerical mathematics to approximate the golden ratio Introduction and historical context I will approximate the golden ratio by iteratively taking ratios of consecutive terms in the Fibonacci sequence. This algorithm lets us peer into the underlying relationship between the Fibonacci sequence and the golden ratio, illuminating a number of interesting patterns.
The golden ratio is a mathematical phenomenon between two numbers, say, a and b. Let a &amp;gt; b, if the ratio of $\frac{a}{b}$ is the same as $\frac{(a+b)}{a}$, then the ratio is ~$\frac{1.</description></item><item><title>Solving a partnership restructuring with computational mathematics</title><link>https://daniel-furman.github.io/posts/solving-a-partnership-restructuring-with-computational-linear-algerba/</link><pubDate>Tue, 20 Apr 2021 21:22:42 -0700</pubDate><guid>https://daniel-furman.github.io/posts/solving-a-partnership-restructuring-with-computational-linear-algerba/</guid><description>Solving a partnership restructuring with computational mathematics Intro Consider a partnership restructuring its assets into majority ownerships among N partners, who previously shared M assets unevenly (debt + value). The asset structure directly reflects the partners&amp;rsquo; differing ownership stakes in the business. Here, I aimed to re-distribute the assets into majority ownerships without altering the partnership stakes, minimizing potential restructuring taxes levied if the asset ownerships changed significantly between partners.</description></item></channel></rss>