---
title: "Extracting metadata from social media captions with machine learning."
date: 2021-12-18
katex: true
markup: "mmark"
---

# A blog post & Web-App exploring social media profiling with machine learning.

Web-app on Heroku: [https://social-media-profiler.herokuapp.com](https://social-media-profiler.herokuapp.com)

**Overview:** This design doc’s purpose is to provide a write-up for the corresponding web-application above. Profiling metadata is valuable for a range of purposes, such as for creating better personalized advertisements online. However, this is clearly a sensitive topic, because people have a range of both positive and negative opinions about internet privacy. Despite existing concerns, companies across the world gather information about people (often customers of the given company) and their personalities as they browse the web, post content on social media platforms, and leave countless other trails of data that reveal features about their identity. To extract such information from social media posts in particular, I propose a solution herein where machine learning models automatically predict personality labels from social media (natural language) posts. The desired final outcome is a system that can label 100’s of metadata labels for a given input that yield an understanding of a person's personality. In the “proof-of-concept” implementation provided, two such labels are predicted: Gender & Platform. The final predictions would be saved and served online within a metadata repository software, preferably a solution that manages such metadata in useful & convenient ways (such as clustering customers with similar personalities, for example). 

**Motivation:** The problem is important to solve now because computer’s are able to analyze textual information significantly faster than human beings. With Natural Language Processing technology we can design computer systems that can rapidly process massive textual databases, which are often in terabytes or petabytes in size. These systems can perform countless tasks with the modern stack of machine learning tools, including analyzing collections of documents to improve business strategy & decision-making, making real-time predictions on new data, writing new content, translating between languages, among many other capabilities. In the coming years, as AI technology advances and becomes more available, profiling systems will likely become more democratized across the world. However, should we be welcoming of such a future? What are the privacy concerns for the average person? What potential policies should be implemented to protect privacy on social media? These are the kinds of questions that I would like this project to elicit from impressions on the Web-App, potentially growing awareness about the need to protect one’s privacy online and on social media platforms.

**Success metrics:** There are two components of success in the final project from the eyes of a company looking to understand their customers better, 1) the accuracy of the machine learners and 2) the usefulness of the metadata tags for profiling customer personalities, and ultimately, for generating business value. For the latter, it would be helpful to frame the project in business goals. This could be reducing costs by replacing human-in-the-loop review of textual information with machine learning systems that automate the process. This could also be increasing advertising revenue by increasing click-through-rates via better personalized ads being recommended to customers.

From the “proof-of-concept” results (see below section Experimentation & Validation), I would preliminarily say that the accuracy for platform classification (~95%) is highly reliable while accuracy for gender profiling (~75%) is not as reliable, but still impressive relative to the baseline (50% on a class balanced dataset). Further testing on more test-data should be performed before proceeding, in addition. For (2) further customization to the use-case would be performed to extract relevant metadata that generates business value. The final metadata labels should adhere to vertical-specific metadata standards and be stored in a more convenient format for computers to parse, such as JSON. For the “proof-of-concept” web-app implementation, the metadata predictions were simply separated into a semi-colon delimited form csv, which is a convenient format to use for metadata storage in this version because it is easy to communicate for the purposes of receiving feedback and clarity.

**Requirements & Constraints:** Different constraints and requirements would exist for a given application of these machine learning models. Functional requirements include customer benefit and perspective (would they be opposed to the privacy concerns of metadata profiling, for example). Non-functional requirements include cost, latency performance, modeling error rate, and data privacy. In terms of privacy, for example, crafting profiling systems that can be used on private data is potentially an ethical and moral grey area. While the “proof-of-concept” models were constructed only on data from public figure verified accounts, the modeling endpoints could potentially be called to make inferences on data that is private in nature.

**Methodology:** 

* Problem statement: The problem was framed as a supervised metadata classification from a given social media profile. 

* Data & Pre-Processing: [[Gender Training Data](https://github.com/daniel-furman/social-media-nlp-app/blob/main/modeling/100-100%20Mix%20Training%20With%20Folds.csv)] [[Gender Testing Data](https://github.com/daniel-furman/social-media-nlp-app/blob/main/modeling/facebook_complete.csv)] [[Platform Data](https://github.com/daniel-furman/social-media-nlp-app/blob/main/modeling/GCP-100-100%20Training%20Platform.csv)] [[Pre-Processing Code](https://github.com/daniel-furman/social-media-nlp-app/blob/main/modeling/Text%20Pre-Processing%20Twitter%20200%20.ipynb)] The models were trained on social media posts scraped from the 300 most followed celebrity Twitter accounts. It was ensured that every celebrity had verified profiles to ensure no privacy or security concerns were violated. Data was collected from Instagram (187 Instagram profiles with 62 posts for each profile, 11,594 individual posts), Twitter (187 Twitter profiles with 200 posts for each profile, 37,400 individual posts), and Facebook (50 Facebook profiles with 20 posts for each profile, 1,000 individual posts). Once all of the raw data was collected, it was preprocessed by removing all retweets and ads, tokenizing with NLTK’s punkt tokenizer, removing all symbols except for letters, numbers, at symbols, and hashtags, and replacing hyperlinks and user tags with <url> and <user> tokens respectively. Posts were concatenated per user with <sep> tokens between each post. The final dataset used for modeling was a stack of all the Twitter captions and Instagram captions (N=372) along with the two labels (Gender and Platform). For the final system, many more labels would be included to capture more representative signals about a customer’s personality. A class-balanced Facebook set of N=50 was used for testing the gender profiling models, while a class-balanced random sample (20%) of the full dataset was used for testing the platform profiling models. The input data for serving the models is simply concatenated captions of social media posts from a given profile.
  
* Techniques & Models: [[Modeling Code](https://github.com/daniel-furman/social-media-nlp-app/blob/main/modeling/Shallow%20Learning%20tf-idf%20Modeling.ipynb)] Three models were trained and analyzed for the task of personality metadata extraction: Support Vector Machines (SVMs), Logistic Regressors (LRs), and Google AutoML models. These algorithms are appropriate since they are SOA for the tasks of Authorship Analyses in the context of social media (Sari et al., 2018) (Radivchev et al., 2019). The models employed the Term Frequency - Inverse Document Frequency (TF-IDF) to featurize the text by considering up to 10,000 bi-grams in the given training set’s vocabulary, the same TF-IDF parameters that Radivchev et al. employed to win the PAN19 Celebrity Profiling challenge (Radivchev et al., 2019). Each data set was separated into 5 class-stratified folds for the purpose of cross-validation training. The models were trained by separating one fold for each iteration to use as a validation set, while the other four were used for training. Each iteration was run through a variety of parameters by using sklearn grid search tuning, with only the best set of hyper-parameters selected per fold. The best performing model in terms of F1 score was selected among the five folds for retraining with the entire data set for the final model. 

* Experimentation & Validation: [[Validation Code](https://github.com/daniel-furman/social-media-nlp-app/blob/main/modeling/FB%20Test.ipynb)] From initial experimentation, Google's Natural Language AutoML performed best for Platform Profiling, while Logistic Regressors trained on TF-IDF bi-gram features performed best for Gender Profiling. As per the modeling test-set, the final Gender Profiling model had an accuracy score of roughly 75% while the Platform Profiling model had an accuracy score of roughly 95%.
  
* Web-App Deployment: [[Web-App Code](https://github.com/daniel-furman/social-media-nlp-app/blob/main/app.py)] The models were converted into callable endpoints to integrate with a “proof-of-concept” web-app implementation. I used AWS lambda functions to run the Gender Profiling Logistic Regression model, a cost-effective serverless function. I used GCP’s native test & deploy capabilities to call the Platform Profiling model. I wrote the website code in Python using the dash library and served the website on a Heroku server. 
  
* Human-in-the-loop: For the final rendition of a metadata extractor (for the purposes of customer personality labeling, for example), human-in-the-loop checks should be put in place to ensure the system remains accurate over time. For example, many alarms and checks should be implemented to ensure the models remain as accurate as possible on new data after live deployment. Moreover, unit testing and model checking should be performed before deployment to ensure the models are viable and performing as a human thinks they should be performing. 
  
**Conclusion:** Metadata extraction from social media profiles is a sensitive topic, since modeling endpoints can be used on private data. Furthermore, there is very little we can do to prevent companies from acquiring our personal data online. As machine learning becomes more democratized, more and more companies will be able to learn more information about its customers on the internet. This begs the question, should anyone keep their profiles public online? Perhaps, for privacy reasons, social media profiles should by default be set to private, with content that is protected by stricter policies to prevent profiling. Otherwise, these systems will be able to glean more and more sensitive information about people online.
  
**References:**
  
Yunita Sari, Mark Stevenson, and Andreas Vlachos. 2018. Topic or style? exploring the most useful features for authorship attribution. In Proceedings of the 27th International Conference on Computational Linguistics, pages 343–353, Santa Fe, New Mexico, USA. Association for Computational Linguistics.
  
Victor Radivchev, Alex Nikolov, and Alexandrina Lam- bova. 2019. Celebrity Profiling using TF-IDF, Lo- gistic Regression, and SVM—Notebook for PAN at CLEF 2019. In CLEF 2019 Labs and Workshops, Notebook Papers. CEUR-WS.org.

